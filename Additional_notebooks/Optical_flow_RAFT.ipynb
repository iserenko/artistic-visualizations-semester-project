{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3679ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credits: https://pytorch.org/vision/main/auto_examples/plot_optical_flow.html?highlight=optical+flow\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f2145dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"edition25_sequence_5.mp4\"\n",
    "\n",
    "DATA_PATH = './lausanne_prix/'\n",
    "OUTPUT_PATH = './Result_OpticalFlow/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd9bdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_input = DATA_PATH + filename\n",
    "video_name = os.path.basename(video_input)\n",
    "\n",
    "video_output = OUTPUT_PATH + video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "716b08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "\n",
    "\n",
    "def plot(imgs, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            img = F.to_pil_image(img.to(\"cpu\"))\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e92b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_video\n",
    "frames, _, _ = read_video(str(video_input), output_format=\"TCHW\")\n",
    "\n",
    "img1_batch = torch.stack([frames[500], frames[550]])\n",
    "img2_batch = torch.stack([frames[501], frames[551]])\n",
    "\n",
    "plot(img1_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2e8effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = torch.Size([2, 3, 608, 720]), dtype = torch.uint8\n",
      "shape = torch.Size([2, 3, 608, 720]), dtype = torch.float32\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.optical_flow import Raft_Small_Weights\n",
    "\n",
    "weights = Raft_Small_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "\n",
    "def preprocess(img1_batch, img2_batch):\n",
    "    #img1_batch = F.resize(img1_batch, size=[608, 720])\n",
    "    #img2_batch = F.resize(img2_batch, size=[608, 720])\n",
    "    return transforms(img1_batch, img2_batch)\n",
    "\n",
    "\n",
    "print(f\"shape = {img1_batch.shape}, dtype = {img1_batch.dtype}\")\n",
    "\n",
    "img1_batch, img2_batch = preprocess(img1_batch, img2_batch)\n",
    "\n",
    "print(f\"shape = {img1_batch.shape}, dtype = {img1_batch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48b129f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type = <class 'list'>\n",
      "length = 12 = number of iterations of the model\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.optical_flow import raft_small\n",
    "\n",
    "# If you can, run this example on a GPU, it will be a lot faster.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = raft_small(weights=Raft_Small_Weights.DEFAULT, progress=False).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "list_of_flows = model(img1_batch.to(device), img2_batch.to(device))\n",
    "print(f\"type = {type(list_of_flows)}\")\n",
    "print(f\"length = {len(list_of_flows)} = number of iterations of the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8adf7f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype = torch.float32\n",
      "shape = torch.Size([2, 2, 608, 720]) = (N, 2, H, W)\n",
      "min = -17.800172805786133, max = 44.04299545288086\n"
     ]
    }
   ],
   "source": [
    "predicted_flows = list_of_flows[-1]\n",
    "print(f\"dtype = {predicted_flows.dtype}\")\n",
    "print(f\"shape = {predicted_flows.shape} = (N, 2, H, W)\")\n",
    "print(f\"min = {predicted_flows.min()}, max = {predicted_flows.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import flow_to_image\n",
    "\n",
    "flow_imgs = flow_to_image(predicted_flows)\n",
    "\n",
    "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
    "img1_batch = [(img1 + 1) / 2 for img1 in img1_batch]\n",
    "\n",
    "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_imgs)]\n",
    "plot(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4241af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# def vizualize_flow(img, flo, save, counter):\n",
    "#     # convert CWH to WHC format and change device if necessary  \n",
    "#     img = img[0].permute(1, 2, 0).cpu().numpy()\n",
    "#     flo = flo[0].permute(1, 2, 0).cpu().numpy()\n",
    " \n",
    "#     # map flow to rgb image\n",
    "#     flo = flow_viz.flow_to_image(flo)\n",
    "#     flo = cv2.cvtColor(flo, cv2.COLOR_RGB2BGR)\n",
    " \n",
    "#     # concatenate, save and show images\n",
    "#     img_flo = np.concatenate([img, flo], axis=0)\n",
    "#     if save:\n",
    "#         cv2.imwrite(f\"demo_frames/frame_{str(counter)}.jpg\", img_flo)\n",
    "#     cv2.imshow(\"Optical Flow\", img_flo / 255.0)\n",
    " \n",
    "#     k = cv2.waitKey(25) & 0xFF\n",
    "#     if k == 27:\n",
    "#         return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5cc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_input)\n",
    "\n",
    "# Obtain frame size information using get() method\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_size = (frame_width,frame_height)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Initialize video writer object\n",
    "output = cv2.VideoWriter(video_output, cv2.VideoWriter_fourcc(*'mp4v'), fps, frame_size)\n",
    "\n",
    "count = 0\n",
    "for i, (img1, img2) in enumerate(zip(frames, frames[1:])):\n",
    "    img1 = torch.stack([img1])\n",
    "    img2 = torch.stack([img2])\n",
    "    #img1 = np.expand_dims(img1, axis=0)\n",
    "    #img2 = np.expand_dims(img2, axis=0)\n",
    "    #print(f\"shape = {img1.shape}, dtype = {img1.dtype}\")\n",
    "    img1, img2 = preprocess(img1, img2)\n",
    "    #print(img1.shape)\n",
    "    #print(img2.shape)\n",
    "    list_of_flows = model(img1.to(device), img2.to(device))\n",
    "    predicted_flow = list_of_flows[-1][0]\n",
    "    flow_img = flow_to_image(predicted_flow).to(\"cpu\")\n",
    "    #print(f\"shape = {flow_img.shape}, dtype = {flow_img.dtype}\")\n",
    "    flow_img = flow_img.numpy()\n",
    "    #print(f\"shape = {flow_img.shape}, dtype = {flow_img.dtype}\")\n",
    "    output.write(flow_img)\n",
    "    \n",
    "    #print(count)\n",
    "    count += 1\n",
    "output.release()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87185766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
